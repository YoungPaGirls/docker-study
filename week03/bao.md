# 컨테이너 실전 구축 및 배포

## 애플리케이션과 시스템 내 단일 컨테이너의 적정 비중

### 컨테이너 1개 = 프로세스 1개?

- 도커 초기에는 컨테이너 1개 = 프로세스 1개를 반드시 지켜야 한다는 사용자가 있어 자주 토론 거리가 됐다
- 과연 이런 생각이 타당할지 보자

**정기적으로 작업을 실행하는 애플리케이션**

- 스케줄러와 작업이 합쳐진 애플리케이션을 만든다면 컨테이너 1개 = 프로세스 1개 원칙을 지킬 수 있을 듯하지만, 스케줄러를 직접 갖춘 애플리케이션만 있는 것은 아니다.
- 보통 스케줄러 기능이 없는 애플리케이션을 사용해 정기 작업을 실행하려면 cron 을 사용한다
- cron 은 1개의 상주 프로세스 형태로 동작하는데, 이것도 역시 하나의 프로세스로 동작한다.
- 컨테이너 1개 = 프로세스 1개 방식을 택한다면 cron 이 1개 컨테이너고 실행되는 작업이 또 1개의 컨테이너를 차지하는 형태로 구성해야 한다
    - 작업 컨테이너 쪽에 작업을 실행하는 트리거 역할을 할 API를 갖추고, cron 컨테이너가 컨테이너 간 통신을 통해 이 API를 호출하는 구조
    - cron 컨테이너에 도커를 구축하고 다시 그 위에서 작업 컨테이너를 실행하는 구조

**~실습~**

- `RUN echo >> /etc/cron.d/cron-example  # 빈 줄 추가 (EOF 오류 방지)`
    - 위처럼 빈 줄을 추가해 줘야 정상적으로 로그가 찍히는데 그 이유는 crontab 에서는 개행이 있어야 정상적인 크론으로 인식을 한다고 한다
    - 그 전에는 cron 파일이라고 인지를 못 하고, 그냥 실행이 안 되는 것.
    - 에러라고 인식도 안 된다. 왜냐하면 크론 파일이라고 인지를 못하기 때문

→ 아무튼 이렇게 하면 컨테이너를 하나만 띄웠지만, cron 과 로그 남기는 작업까지 2개의 프로세스를 한번에 할 수 있다. 억지로 별도의 컨테이너로 분리하는 것보다 깔끔하다.

### 컨테이너 1개에 하나의 관심사

- 공식 문서에서는 컨테이너는 하나의 관심사에만 집중해야 한다고 나온다
- 각 컨테이너가 맡을 역할을 적절히 나누고, 그 역할에 따라 배치한 컨테이너를 복제해도 전체 구조에서 부작용이 일어나지 않는가를 따져가며 시스템을 설계해야 한다

## 컨테이너의 이식성

### 커널 및 아키텍처의 차이

- 컨테이너를 실행하려면 호스트가 특정 CPU 아키텍처 혹은 운영 체제를 사용해야 한다
- 알파인 리눅스를 기반으로 도커 이미지를 빌드하거나 사용하는데, 이건 x86_64 아키텍처에서 실행하는 것을 전제로 만들어진 것이고, 다른 아키텍처에서 동작하는 도커에서는 실행이 보장되지 않는다.

### 라이브러리와 동적 링크 문제

- 정적 링크는 애플리케이션에 사용된 라이브러리를 내부에 포함하는 형태이다.
- 동적 링크는 애플리케이션을 실행할 때 라이브러리가 링크된다. 애플리케이션을 실행할 호스트에서 라이브러리를 갖춰야 한다.
- 동적 링크는 컨테이너 안에서 애플리케이션을 빌드하는 것보다 부하가 가볍기 때문에 괜찮은 방법 같지만, 문제가 하나 있다.
- 예를 들어, CI 와 컨테이너에서 사용하는 표준 C 라이브러리가 다르다면 CI 에서 빌드한 뒤 복사해온 애플리케이션은 컨테이너에서 동작하지 않을 것이다.
- 정적 링크는 실행 파일의 크기가 커진다
- multi-stage builds 라는 매커니즘을 제시했는데, 빌드용과 실행용으로 컨테이너를 분리해서 사용하는 것이다.
- 근데 이렇게 해도 이식성이 완벽해지는 것은 아니기 때문에, 절대적인 것이 아님을 이해해야 한다.

# 도커 친화적인 애플리케이션

- 이식성 높은 애플리케이션을 구축하기 위해서는 어떤 요소가 필요할까?
- 도커 컨테이너 형태로 실행되는 애플리케이션의 동작을 제어하는 방법
    - 실행 시 인자
    - 설정 파일
    - 애플리케이션 동작을 환경 변수로 제어
    - 설정 파일에 환경 변수를 포함

### 실행 시 인자를 활용

- 이 방법은 외부에서 값을 주입받을 수 있다
- 하지만 인자가 너무 많아지면 애플리케이션에서 인자를 내부 변수로 매핑해주는 처리가 복잡해진다.

### 설정 파일 사용

- 특정 환경에 대한 설정 파일을 도커 이미지에 포함시키면, 이식성을 해치게 된다
- 호스트에 대한 의존성이 생길 수 있다

### 애플리케이션 동작을 환경 변수로 제어

- 매번 이미지를 다시 빌드하지 않아도 된다
- 컴포즈를 사용하는 경우, docker-compose.yml 파일의 env 속성에 기술해 관리한다.
- 하지만, 계층 구조를 갖기 어렵다.

### 설정 파일에 환경 변수를 포함

- 도커에서 이 방법을 추천한다는데, (이게 무슨 말임…?)
    - 예를 들어 스프링 프레임워크 properties 파일은 환경 변수를 포함할 수 있는데,
    기존에는 환경 가짓수만큼 설정 파일을 만들어야 했지만, 환경 변수를 포함함에 따라 이 파일을 템플릿으로 사용할 수 있게 됐다.

# 퍼시스턴스 데이터를 다루는 방법

- 도커 컨테이너가 실행 중에 작성 혹은 수정된 파일은 호스트 쪽 파일 시스템에 마운트되지 않는 한, 컨테이너가 파기될 때 호스트에서 함께 삭제된다.
- stateful 유형이라면 파기된 컨테이너를 완전히 동일하게 재현하기가 쉽지 않다.
- 상태를 갖는 애플리케이션을 운영하려면 ㄷ에ㅣ터 볼륨을 사용해야 한다.

## 데이터 볼륨

- 컨테이너 안의 디렉터리를 디스크에 퍼시스턴스 데이터로 남기기 위한 메커니즘이다.
- 호스트와 컨테이너 사이의 디렉터리 공유 및 재사용 기능을 제공한다.
- 새로 컨테이너를 생성해도 같은 데이터 볼륨을 계속 사용할 수 있다.
- -v 옵션을 사용하면 된다
- 컨테이너 안의 설정 파일을 쉽게 수정할 수 있지만, 호스트 안의 특정 경로에 의존성이 생기기 때문에 데이터 볼륨을 잘못 다루면 애플리케이션에 부정적 영향을 미칠 수 있다.
- 따라서 이식성 면에서는 개선의 여지가 있는 기법이다.

## 데이터 볼륨 컨테이너

- 컨테이너 간에 디렉터리를 공유한다
- 데이터를 저장하는 것만이 목적이다
- 데이터 볼륨은 호스트 쪽 특정 디렉터리에 의존성을 갖지만, 컨테이너 볼륨은 도커에서 관리하는 영역인 호스트 머신의 /var/lib/docker/volumes/ 아래에 위치한다. 데이터 볼륨 컨테이너는 도커가 관리하는 디렉터리 영역에만 영향을 미친다.
- 컨테이너에 미치는 영향을 최소한으로 억제한다.
- 호스트에 아는 것이 없어도 사용할 수 있다. 데이터의 결합이 느슨하다.

### 데이터 볼륨에 MySQL 데이터 저장하기

- 데이터 볼륨 컨테이너는 데이터를 저장하는 것만을 목적으로 하는 컨테이너이기 때문에, 작은 이미지를 사용하는 것이 효과적이다.

**~ 실습 ~**

- docker 옵션에 —rm 을 붙였기 때문에 정지와 함께 컨테이너가 삭제되는데, 새로운 컨테이너를 실행하고 select 문을 작성하면 추가했던 데이터가 남아있다.

**데이터 익스포트 및 복원**

- 데이터 볼륨은 좋은 기능이지만, 그 범위는 같은 호스트 안으로 제한된다.
- 데이터 볼륨 컨테이너에서 사용하던 데이터를 다른 도커 호스트로 이전해야 하는 경우가 생긴다.
- 그럼 그 데이터를 익스포트 한 뒤 호스트로 꺼내야 한다.

**~ 실습 ~**

- tar 파일로 꺼내온 것을 새로운 데이터 볼륨 컨테이너를 만들고, 압축 파일을 풀어주면 된다.

# 컨테이너 배치 전략

- 많은 트래픽을처리할 수 있는 실용적인 시스템은 여러 컨테이너가 각기 다른 호스트에 배치되는 경우가 많다.

## 도커 스웜

- 여러 도커 호스트를 클러스터로 묶어주는 컨테이너 오케스트레이션 도구의 한 종류이다.
- 오케스트레이션 도구를 도입하면 조율에 수고를 절감하고, 호스트가 여러 대로 나뉘어있다는 점을 신경 쓰지 않고 클러스터를 투명하게 다룰 수 있다.
- 컴포즈
    - 여러 컨테이너로 구성된 도커 애플리케이션을 관리 (주로 단일 호스트)
    - docker-compose
- 스웜
    - 클러스터 구축 및 관리 (주로 멀티 호스트)
    - docker swarm
- 서비스
    - 스웜에서 클러스터 안의 서비스 (컨테이너 하나 이상의 집합) 를 관리
        - docker service
- 스택
    - 스웜에서 여러 개의 서비스를 합한 전체 애플리케이션을 관리
        - docker stack

### 여러 대의 도커 호스트로 스웜 클러스터 구성하기

- 호스트를 여러대 사용하려면 특정 서비스를 사용해야 함
- docker in docker 를 사용하면 도커 컨테이너 안에서 도커 호스트를 실행할 수 있다.

**~실습~**

- restry: 도커 레지스트리 역할 (실제로는 보통 ecr 같은 걸 쓰긴 함)
- manager: 스웜 클러스터 전체를 제어.
    - 여러 대 실행되는 도커 호스트(worker) 에 서비스가 담긴 컨테이너를 적절히 배치
- docker:24-dind 로 수정해 줘야 잘 됨. arm64 에 호환되지 않는다고 함 (지피티)
- 그냥 docker node ls 하면 안 되고 `docker container exec -it manager docker node ls` 를 해야 함 왜냐하면 그냥 worker 노드이기 때문. manager 에서 실행해야 node 를 볼 수 이씀
- 근데 안 돼서 시발.. 못했음
    - 5000번은 좀비처럼 계속 살아나고
    - 5001번으로 하려니
    
    f53a70d5dc65: Pushed
    failed commit on ref "manifest-sha256:ab14422623f57dae92a24a2cb4f1a15df1a842ac123c63b9dd688f708b1d972a": unexpected status from PUT request to http://localhost:5001/v2/example/echo/manifests/latest: 400 Bad Request
    
    - 계속 이 에러가 나서 못 함… ㅅㅂ 토나와서 못했음
- 아무튼 이게 됐었다면 레지스트리로 쓰고 push 하고 pull 받고 하는 거였음

## 서비스

- 애플리케이션을 구성하는 일부 컨테이너를 제어하기 위한 단위

**~ 실습 ~**

- docker service create 명령으로 생성
- docker service scale 명령으로 컨테이너 수를 늘리거나 줄일 수 있음
- 이 지시로 자동으로 복자하고, 여러 노드에 배치함. 이게 스케일인 스케일 아웃임

## 스택

- 하나 이상의 서비스를 그룹으로 묶은 단위다.
- 스웜에서 동작하는 스케일 인, 스케일 아웃, 제약 조건 부여가 가능한 컴포즈이다.
- overlay 네트워크를 설정하지 않으면 스택마다 서로 다른 overlay 네트워크를 생성하고, 그 안에 서비스 그룹이 속한다.
- 서비스가 다른 overlay 는 통신도 불가능하다.
- 이 문제를 해결하려면 클라이언트와 대상 서비스가 같은 overlay 네트워크에 있어야 한다.
- 스택 역시 replica 수를 조정할 수 있다
    - 레플리카(Replica)란 동일한 컨테이너를 여러 개 생성하여 **부하 분산과 가용성을 높이는 기능**이다.
    - 예를 들어, `docker service create --replicas 3 myapp`을 실행하면 동일한 `myapp` 컨테이너가 **3개** 실행됨.
    - 이렇게 하면 **한 개의 컨테이너가 죽더라도 다른 컨테이너가 동일한 역할을 수행할 수 있어 장애에 강해짐**.

## visualizeer 를 사용해 컨테이너 배치 시각화하기

- 스웜 클러스터에 컨테이너 그룹이 어떤 노드에 어떻게 배치됐는지 시각화해 주는 visualizer 라는 애플리케이션이 있다.

### 스웜 클러스터 외부에서 서비스 사용하기

- echo_nginx  서비스는 여러 컨테이너가 여러 노드에 흩어져 배치돼 있기 때문에 이런 방법을 사용할 수 없다. 호스트에서 이 서비스에 접근하려면 어떻게 해야 할까?
- 프록시 서버가 있어야 하는데, HAProxy 를 ㅍ록시 서버로 사용해 스웜 클러스터 외부에서  echo_nginx 서비스에 접근할 수 있개 할 수 있다
- dockercloud/haproxy 로 만든 컨테이너는 컨테이너 외부에서 서비스에 접근할 수 있게 해 주는 다리 역할(ingress) 외에도 서비스가 배치된 노드에 로드 밸런싱 기능을 제공한다.
- 호스트의 포트 8000이 manager 컨테이너의 포트 80, ingress의 HAProxy로 포트포워딩 돼서 [localhost:8000](http://localhost:8000/)이 echo_nginx:80에 접근할 수 있게 된다.
    - **이게 무슨 말이냐면**,
        1. **호스트(OS)의 8000번 포트** → **도커 manager 컨테이너의 80번 포트로 연결됨**
        2. **manager 컨테이너의 80번 포트** → **HAProxy 컨테이너의 80번 포트로 연결됨**
        3. **HAProxy 컨테이너** → Swarm 내부의 `echo_nginx:80`으로 트래픽을 전달함.
    - 즉, 사용자가 `localhost:8000`으로 요청을 보내면, **이 요청이 HAProxy를 통해 Swarm 내부의 `echo_nginx` 컨테이너로 전달되는 구조**.
    - **HAProxy는 여러 개의 `echo_nginx` 컨테이너 중 하나로 트래픽을 분산시키는 역할을 함.**